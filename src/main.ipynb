{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddd0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425c95b",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9723fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(f):\n",
    "    match = re.search(r'\\d+', f)\n",
    "    return int(match.group()) if match else float('inf')\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted( os.listdir(root_dir), key=extract_number)\n",
    "            #[f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.jpeg', '.png'))],\n",
    "            #key=lambda x: int(os.path.splitext(x)[0])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            print(f\"Error loading {img_name}, skipping...\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, img_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ea939",
   "metadata": {},
   "source": [
    "# 2. Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de08dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a091942",
   "metadata": {},
   "source": [
    "# 3. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c262f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=200):\n",
    "    #model = models.efficientnet_b4(pretrained=True)\n",
    "    from torchvision.models import EfficientNet_B4_Weights\n",
    "    weights = EfficientNet_B4_Weights.DEFAULT\n",
    "    model = models.efficientnet_b4(weights=weights)\n",
    "\n",
    "    \n",
    "    # Freeze base layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Replace classifier\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cee59",
   "metadata": {},
   "source": [
    "# 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6dc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Initialize\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = create_model().to(device)\n",
    "    \n",
    "    # Data Loaders\n",
    "    train_dataset = ImageFolder(root='../data/train', transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Test Loader\n",
    "    test_dataset = TestDataset(root_dir='../data/test', transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Training Setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a41cd",
   "metadata": {},
   "source": [
    "# 5. Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff87bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, names in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            filenames.extend(names)\n",
    "    \n",
    "    # Create mapping\n",
    "    class_to_idx = ImageFolder(root='../data/train').class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'ID': filenames,\n",
    "        'label': [idx_to_class[p] for p in predictions]\n",
    "    })\n",
    "    \n",
    "    # Sort numerically\n",
    "    df['num'] = df['ID'].apply(lambda x: int(os.path.splitext(x)[0]))\n",
    "    df = df.sort_values('num').drop('num', axis=1)\n",
    "    df.to_csv('../submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4240d",
   "metadata": {},
   "source": [
    "# Main Execution of data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a1f53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 295/295 [20:14<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.1473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 75/75 [04:46<00:00,  3.81s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'image1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m trained_model = train_model()\n\u001b[32m      3\u001b[39m test_loader = DataLoader(\n\u001b[32m      4\u001b[39m     TestDataset(root_dir=\u001b[33m'\u001b[39m\u001b[33m../data/test\u001b[39m\u001b[33m'\u001b[39m, transform=test_transform),\n\u001b[32m      5\u001b[39m     batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mgenerate_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgenerate_submission\u001b[39m\u001b[34m(model, test_loader, device)\u001b[39m\n\u001b[32m     19\u001b[39m df = pd.DataFrame({\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m: filenames,\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: [idx_to_class[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[32m     22\u001b[39m })\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Sort numerically\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m df = df.sort_values(\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m).drop(\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m df.to_csv(\u001b[33m'\u001b[39m\u001b[33m../submission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgenerate_submission.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     19\u001b[39m df = pd.DataFrame({\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m: filenames,\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: [idx_to_class[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[32m     22\u001b[39m })\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Sort numerically\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     26\u001b[39m df = df.sort_values(\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m).drop(\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m df.to_csv(\u001b[33m'\u001b[39m\u001b[33m../submission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'image1'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model()\n",
    "    test_loader = DataLoader(\n",
    "        TestDataset(root_dir='../data/test', transform=test_transform),\n",
    "        batch_size=32, shuffle=False, num_workers=4\n",
    "    )\n",
    "    generate_submission(trained_model, test_loader, \n",
    "                       device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d4bb6",
   "metadata": {},
   "source": [
    "# Activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3ed16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # z = np.clip(z, -500, 500)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def deriv_sigmoid(Z):\n",
    "    # print(Z.min(), Z.max())\n",
    "    return Z*(1-Z)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # for stability\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "def compute_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss\n",
    "    AL: predictions (softmax output), shape (200, m)\n",
    "    Y: true labels (one-hot), shape (200, m)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y * np.log(AL + 1e-9)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb323f56",
   "metadata": {},
   "source": [
    "# Read from csv file or data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30690e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6aeab1",
   "metadata": {},
   "source": [
    "# Neurons in layers stored in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_sizes(n, point1, point2):\n",
    "    \"\"\"\n",
    "    Returns the list of layer sizes (input + hidden + output)\n",
    "    point1 and point2 are (layer_index, neuron_count) pairs\n",
    "    \"\"\"\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    slope = (y2 - y1) / (x2 - x1)\n",
    "    intercept = y1 - slope * x1\n",
    "    \n",
    "    layer_sizes = []\n",
    "    for i in range(n + 1):  # Including input and output layer\n",
    "        neurons = int(round(slope * i + intercept))\n",
    "        layer_sizes.append(neurons)\n",
    "    return layer_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ce612",
   "metadata": {},
   "source": [
    "# Init Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa35997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(n, point1, point2, seed=42):\n",
    "    \"\"\"\n",
    "    Initializes weights and biases using the layer sizes.\n",
    "    Returns a dictionary of parameters.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    layer_sizes = get_layer_sizes(n, point1, point2)\n",
    "    parameters = {}\n",
    "\n",
    "    for l in range(1, len(layer_sizes)):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_sizes[l], layer_sizes[l-1]) * np.sqrt(2. / layer_sizes[l-1])\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_sizes[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7bf1d",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, parameters):\n",
    "    \"\"\"\n",
    "    x: input column vector of shape (150528, 1)\n",
    "    parameters: dictionary of W and b\n",
    "    Returns final output (softmax probabilities) and cache of intermediate values.\n",
    "    \"\"\"\n",
    "    A = x\n",
    "    cache = {\"A0\": x}\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        Z = parameters[f\"W{l}\"] @ A + parameters[f\"b{l}\"]\n",
    "        A = relu(Z)\n",
    "        cache[f\"Z{l}\"] = Z\n",
    "        cache[f\"A{l}\"] = A\n",
    "\n",
    "    # Output layer (softmax)\n",
    "    ZL = parameters[f\"W{L}\"] @ A + parameters[f\"b{L}\"]\n",
    "    AL = softmax(ZL)\n",
    "    cache[f\"Z{L}\"] = ZL\n",
    "    cache[f\"A{L}\"] = AL\n",
    "\n",
    "    return AL, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43896735",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Returns gradients dW, db for each layer\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    # Output layer gradient\n",
    "    AL = cache[f\"A{L}\"]\n",
    "    dZL = AL - Y\n",
    "    grads[f\"dW{L}\"] = (1 / m) * dZL @ cache[f\"A{L-1}\"].T\n",
    "    grads[f\"db{L}\"] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "\n",
    "    dA_prev = parameters[f\"W{L}\"].T @ dZL\n",
    "\n",
    "    # Hidden layers\n",
    "    for l in reversed(range(1, L)):\n",
    "        dZ = dA_prev * relu_derivative(cache[f\"Z{l}\"])\n",
    "        grads[f\"dW{l}\"] = (1 / m) * dZ @ cache[f\"A{l-1}\"].T\n",
    "        grads[f\"db{l}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        if l > 1:\n",
    "            dA_prev = parameters[f\"W{l}\"].T @ dZ\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933d827",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b7b0d",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, n, point1, point2, epochs=100, learning_rate=0.01, print_loss=True):\n",
    "    \"\"\"\n",
    "    X: input data (150528, m)\n",
    "    Y: one-hot labels (200, m)\n",
    "    \"\"\"\n",
    "    parameters = init_parameters(n, point1, point2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward Propagation\n",
    "        AL, cache = forward_propagation(X, parameters)\n",
    "\n",
    "        # Loss\n",
    "        loss = compute_loss(AL, Y)\n",
    "\n",
    "        # Backward Propagation\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "\n",
    "        # Update Parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_loss and epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d0e70",
   "metadata": {},
   "source": [
    "# Testing and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5806059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, parameters):\n",
    "    \"\"\"\n",
    "    X: input image(s), shape (150528, m)\n",
    "    Returns: predictions, shape (m,)\n",
    "    \"\"\"\n",
    "    AL, _ = forward_propagation(X, parameters)\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(index, X, Y, parameters):\n",
    "    \"\"\"\n",
    "    index: index of the image in dataset\n",
    "    X: shape (150528, m)\n",
    "    Y: shape (200, m) — one-hot labels\n",
    "    parameters: learned parameters\n",
    "    \"\"\"\n",
    "    current_image = X[:, index:index+1]  # shape (150528, 1)\n",
    "    prediction = make_predictions(current_image, parameters)[0]\n",
    "    label = np.argmax(Y[:, index])\n",
    "\n",
    "    print(\"Prediction:\", prediction)\n",
    "    print(\"Label:\", label)\n",
    "    \n",
    "    img = current_image.reshape(224, 224, 3)\n",
    "    plt.imshow(img.astype(np.uint8))\n",
    "    plt.title(f\"Predicted: {prediction}, Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211773ed",
   "metadata": {},
   "source": [
    "# Accuracy vs no of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    predictions: shape (m,)\n",
    "    labels: shape (200, m) — one-hot\n",
    "    \"\"\"\n",
    "    true_labels = np.argmax(labels, axis=0)\n",
    "    return np.mean(predictions == true_labels)\n",
    "\n",
    "def plot_accuracy_vs_layers(X, Y, layer_range, point1, point2, epochs=50, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    X: input data (150528, m)\n",
    "    Y: one-hot labels (200, m)\n",
    "    layer_range: list or range of layer counts to try (e.g., range(2, 9))\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "\n",
    "    for n_layers in layer_range:\n",
    "        print(f\"Training with {n_layers} layers...\")\n",
    "        parameters = train_model(X, Y, n_layers, point1, point2, epochs=epochs, learning_rate=learning_rate, print_loss=False)\n",
    "        preds = make_predictions(X, parameters)\n",
    "        acc = compute_accuracy(preds, Y)\n",
    "        print(f\"Accuracy for {n_layers} layers: {acc:.4f}\")\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(list(layer_range), accuracies, marker='o')\n",
    "    plt.title(\"Accuracy vs Number of Layers\")\n",
    "    plt.xlabel(\"Number of Layers\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401e455",
   "metadata": {},
   "source": [
    "# Main function for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41089f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
